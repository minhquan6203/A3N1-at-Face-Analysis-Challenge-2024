{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["fCfuUXYwLW54","-GRSnF7_L4MJ"],"gpuType":"T4","authorship_tag":"ABX9TyN7V+zwVPu6AD0TPDxwSLg/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U6cUy3fkLHbC","executionInfo":{"status":"ok","timestamp":1705323106982,"user_tz":-420,"elapsed":2058,"user":{"displayName":"Quân Nguyễn Văn","userId":"00163078239839299976"}},"outputId":"36e199f3-6f82-42ff-9935-4c3c80ceda1a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Face-Analysis-Challenge'...\n","remote: Enumerating objects: 127, done.\u001b[K\n","remote: Counting objects: 100% (127/127), done.\u001b[K\n","remote: Compressing objects: 100% (75/75), done.\u001b[K\n","remote: Total 127 (delta 74), reused 101 (delta 48), pack-reused 0\u001b[K\n","Receiving objects: 100% (127/127), 25.60 KiB | 4.27 MiB/s, done.\n","Resolving deltas: 100% (74/74), done.\n"]}],"source":["!git clone https://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@github.com/minhquan6203/Face-Analysis-Challenge"]},{"cell_type":"markdown","source":["# prepare data"],"metadata":{"id":"fCfuUXYwLW54"}},{"cell_type":"markdown","source":["# data ban đầu"],"metadata":{"id":"96LNT1mdo98b"}},{"cell_type":"code","source":["#https://drive.google.com/file/d/1Qq5PLe0ntyxYKSKZecVwTt9TaaQ-lL8A/view?usp=sharing\n","!curl -L -o 'data1.zip' 'https://drive.usercontent.google.com/download?id=1Qq5PLe0ntyxYKSKZecVwTt9TaaQ-lL8A&export=download&authuser=1&confirm=t'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bf3HmuuFLWe5","executionInfo":{"status":"ok","timestamp":1705323230782,"user_tz":-420,"elapsed":115862,"user":{"displayName":"Quân Nguyễn Văn","userId":"00163078239839299976"}},"outputId":"a91a0592-491d-428c-8626-c04668b34e3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 17.0G  100 17.0G    0     0   154M      0  0:01:53  0:01:53 --:--:--  214M\n"]}]},{"cell_type":"code","source":["#https://drive.google.com/file/d/18J2DIDUX2dvX3_pBGWlbF4Dgi2I7O3Qm/view?usp=sharing\n","!curl -L -o 'label1.csv' 'https://drive.usercontent.google.com/download?id=18J2DIDUX2dvX3_pBGWlbF4Dgi2I7O3Qm&export=download&authuser=1&confirm=t'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NPfTfeqVLbQp","executionInfo":{"status":"ok","timestamp":1705323231348,"user_tz":-420,"elapsed":571,"user":{"displayName":"Quân Nguyễn Văn","userId":"00163078239839299976"}},"outputId":"1979cf6f-da53-4a1f-97f0-67cee4b71991"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 2260k  100 2260k    0     0  1873k      0  0:00:01  0:00:01 --:--:-- 1874k\n"]}]},{"cell_type":"markdown","source":["# data bonus"],"metadata":{"id":"_L1XMqs_pALR"}},{"cell_type":"code","source":["#https://drive.google.com/file/d/1mgRpPyh2GzdFl2ZsrEOJfIhYx-aBiOoT/view?usp=sharing\n","!curl -L -o 'data2.zip' 'https://drive.usercontent.google.com/download?id=1mgRpPyh2GzdFl2ZsrEOJfIhYx-aBiOoT&export=download&authuser=1&confirm=t'"],"metadata":{"id":"n1T8mmUDpCl2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#https://drive.google.com/file/d/14HkR-x1445_mE7ipfqsm5nrzlHFIqOZR/view?usp=sharing\n","!curl -L -o 'label2.csv' 'https://drive.usercontent.google.com/download?id=14HkR-x1445_mE7ipfqsm5nrzlHFIqOZR&export=download&authuser=1&confirm=t'"],"metadata":{"id":"RRkZBqfSpID-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## gộp 2 data"],"metadata":{"id":"FXG7fzmspgtf"}},{"cell_type":"code","source":["%cd /content/\n","!unzip -qq data1.zip\n","!unzip -qq data2.zip"],"metadata":{"id":"o_kDoZ3dpSU4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import shutil\n","img_data1_path='mnt/md0/projects/sami-hackathon/private/data'\n","img_data2_path='data'\n","os.makedirs('all_img',exist_ok=True)\n","for file in os.listdir(img_data1_path):\n","  shutil.move(os.path.join(img_data1_path,file),'all_img')\n","\n","for file in os.listdir(img_data2_path):\n","  shutil.move(os.path.join(img_data2_path,file),'all_img')"],"metadata":{"id":"ib26rD9IpXr3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","data1=pd.read_csv('label1.csv')\n","data2=pd.read_csv('label2.csv')\n","data2['race'] = data2['race'].replace('Mongolid', 'Mongoloid')\n","data = pd.concat([data1, data2], axis=0)\n","data['id']=data.index\n","data.to_csv('labels.csv',index=False)"],"metadata":{"id":"9Z1oKE_Dpbxf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## chia train dev"],"metadata":{"id":"Cz0hVzycpx8q"}},{"cell_type":"code","source":["import pandas as pd\n","from PIL import Image\n","import os\n","from tqdm import tqdm\n","\n","def crop_img(folder_path, df, crop_folder):\n","    os.makedirs(\"data\", exist_ok=True)\n","    os.makedirs(crop_folder, exist_ok=True)\n","    img_name_list=[]\n","    age_list=[]\n","    race_list=[]\n","    masked_list=[]\n","    skintone_list=[]\n","    emotion_list=[]\n","    gender_list=[]\n","    for idx, row in tqdm(df.iterrows(), total=len(df), desc='Processing images'):\n","        img_path = os.path.join(folder_path, row['file_name'])\n","        img = Image.open(img_path)\n","        bbox = row['bbox'].replace('[','').replace(']','').replace(' ','').strip().split(',')\n","        x1, y1, x2, y2 = float(bbox[0]), float(bbox[1]), float(bbox[2]), float(bbox[3])\n","        x_min, y_min, x_max, y_max =  x1, y1, x2 + x1, y2 + y1\n","        cropped_img = img.crop([x_min, y_min, x_max, y_max])\n","        cropped_img.save(os.path.join(crop_folder, f\"{idx}.jpg\"))\n","\n","        img_name_list.append(f\"{idx}.jpg\")\n","        age_list.append(row['age'])\n","        race_list.append(row['race'])\n","        masked_list.append(row['masked'])\n","        skintone_list.append(row['skintone'])\n","        emotion_list.append(row['emotion'])\n","        gender_list.append(row['gender'])\n","\n","    df=pd.DataFrame({'crop_name':img_name_list,\n","                     'age':age_list,\n","                     'race':race_list,\n","                     'masked':masked_list,\n","                     'skintone':skintone_list,\n","                     'emotion':emotion_list,\n","                     'gender':gender_list})\n","    df.to_csv('data/crop_image_info.csv',index=False)\n","\n","data = pd.read_csv('labels.csv')\n","folder_path='all_img'\n","crop_folder='crop_image'\n","crop_img(folder_path,data,crop_folder)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tPzSlkKrLeoz","executionInfo":{"status":"ok","timestamp":1705324044637,"user_tz":-420,"elapsed":565450,"user":{"displayName":"Quân Nguyễn Văn","userId":"00163078239839299976"}},"outputId":"53651c7c-296e-4a4a-bca9-612f4bfb6fb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Processing images: 100%|██████████| 15310/15310 [09:24<00:00, 27.10it/s]\n"]}]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","os.makedirs(\"data\", exist_ok=True)\n","from numpy.random import RandomState\n","rng = RandomState(1234)\n","data=pd.read_csv('data/crop_image_info.csv')\n","train = data.sample(frac=0.8, random_state=rng)\n","dev = data.loc[~data.index.isin(train.index)]\n","train.to_csv('data/train.csv',index=False)\n","dev.to_csv('data/dev.csv',index=False)\n","print(train.shape)\n","print(dev.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"evA3vgr9Lsi0","executionInfo":{"status":"ok","timestamp":1705324044638,"user_tz":-420,"elapsed":6,"user":{"displayName":"Quân Nguyễn Văn","userId":"00163078239839299976"}},"outputId":"21cc703c-eb64-4141-dadc-79753d6deb9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(12248, 7)\n","(3062, 7)\n"]}]},{"cell_type":"markdown","source":["# training"],"metadata":{"id":"-GRSnF7_L4MJ"}},{"cell_type":"code","source":["!python /content/Face-Analysis-Challenge/main.py --config /content/Face-Analysis-Challenge/config/trans_config.yaml"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z4ormfPDL366","executionInfo":{"status":"ok","timestamp":1705324086855,"user_tz":-420,"elapsed":41354,"user":{"displayName":"Quân Nguyễn Văn","userId":"00163078239839299976"}},"outputId":"906923a2-b897-4f60-a0fa-890b166383a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-01-15 13:07:51.036268: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-01-15 13:07:51.036384: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-01-15 13:07:51.157536: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-01-15 13:07:53.616417: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Training started...\n","preprocessor_config.json: 100% 160/160 [00:00<00:00, 776kB/s]\n","config.json: 100% 502/502 [00:00<00:00, 1.94MB/s]\n","/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n","  warnings.warn(\n","pytorch_model.bin: 100% 346M/346M [00:01<00:00, 194MB/s]\n","Reading training data...\n","Reading validation data...\n","first time training!!!\n","  6% 11/192 [00:19<04:40,  1.55s/it]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x79491abb93f0>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n","    self._shutdown_workers()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1442, in _shutdown_workers\n","    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 149, in join\n","    res = self._popen.wait(timeout)\n","  File \"/usr/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n","    if not wait([self.sentinel], timeout):\n","  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n","    ready = selector.select(timeout)\n","  File \"/usr/lib/python3.10/selectors.py\", line 416, in select\n","    fd_event_list = self._selector.poll(timeout)\n","KeyboardInterrupt: \n","  6% 11/192 [00:19<05:22,  1.78s/it]\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/pathlib.py\", line 625, in __str__\n","    return self._str\n","AttributeError: 'PosixPath' object has no attribute '_str'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/content/Face-Analysis-Challenge/main.py\", line 28, in <module>\n","    main(args.config)\n","  File \"/content/Face-Analysis-Challenge/main.py\", line 15, in main\n","    task_train.training()\n","  File \"/content/Face-Analysis-Challenge/task/training.py\", line 84, in training\n","    logits, loss = self.base_model(item['image_name'], age,race,masked,skintone,emotion,gender)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/Face-Analysis-Challenge/model/model_trans_baseline.py\", line 30, in forward\n","    embedded_vision, vison_mask = self.vision_embedding(images)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/Face-Analysis-Challenge/vision_module/vision_embedding.py\", line 44, in forward\n","    processed_images=self.processor(images=[self.load_image(image_name) for image_name in image_names],return_tensors=\"pt\").to(self.device)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py\", line 549, in __call__\n","    return self.preprocess(images, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/vit/image_processing_vit.py\", line 245, in preprocess\n","    images = [\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/vit/image_processing_vit.py\", line 246, in <listcomp>\n","    self.resize(image=image, size=size_dict, resample=resample, input_data_format=input_data_format)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/vit/image_processing_vit.py\", line 137, in resize\n","    return resize(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\", line 327, in resize\n","    image = to_pil_image(image, do_rescale=do_rescale, input_data_format=input_data_format)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/image_transforms.py\", line 179, in to_pil_image\n","    requires_backends(to_pil_image, [\"vision\"])\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1245, in requires_backends\n","    failed = [msg.format(name) for available, msg in checks if not available()]\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1245, in <listcomp>\n","    failed = [msg.format(name) for available, msg in checks if not available()]\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 697, in is_vision_available\n","    package_version = importlib.metadata.version(\"Pillow\")\n","  File \"/usr/lib/python3.10/importlib/metadata/__init__.py\", line 996, in version\n","    return distribution(distribution_name).version\n","  File \"/usr/lib/python3.10/importlib/metadata/__init__.py\", line 627, in version\n","    return self.metadata['Version']\n","  File \"/usr/lib/python3.10/importlib/metadata/__init__.py\", line 605, in metadata\n","    self.read_text('METADATA')\n","  File \"/usr/lib/python3.10/importlib/metadata/__init__.py\", line 927, in read_text\n","    return self._path.joinpath(filename).read_text(encoding='utf-8')\n","  File \"/usr/lib/python3.10/pathlib.py\", line 1134, in read_text\n","    with self.open(mode='r', encoding=encoding, errors=errors) as f:\n","  File \"/usr/lib/python3.10/pathlib.py\", line 1119, in open\n","    return self._accessor.open(self, mode, buffering, encoding, errors,\n","  File \"/usr/lib/python3.10/pathlib.py\", line 632, in __fspath__\n","    return str(self)\n","  File \"/usr/lib/python3.10/pathlib.py\", line 625, in __str__\n","    return self._str\n","KeyboardInterrupt\n","^C\n"]}]}]}